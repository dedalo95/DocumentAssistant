{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ef9624-1e79-4038-be86-4302b72f25a5",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "In this notebook will be extracted data from Wikipedia or from a custom folder.\n",
    "A Vector DB will be created (Chroma or Faiss) and ChatGPT will answer questions about the topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c83be-a4bb-404e-b4af-c9f6a9ec8d88",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e147530-4fca-4ca5-aa4b-a5eb9afc28e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Use Wikipedia as data source\n",
    "In this example you will donwload data from wikipedia and will use it for building the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08028c-5585-48bd-9add-c520f354f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5021cc-761b-4f21-a0bf-6e95036b4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from Wikipedia\n",
    "search_term = \"Stanley Kubrick\"\n",
    "#Choose how many documents we want to load\n",
    "docs = WikipediaLoader(query=search_term, load_max_docs=1).load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823379d2-7f00-4a96-9191-6e5f5712cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split documents into chunks\n",
    "#set up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100, #how many chars will be in a single chunk\n",
    "    chunk_overlap = 20, #how many chars we want to overlap between chunks\n",
    "    is_separator_regex = False\n",
    ")\n",
    "#split data\n",
    "data = text_splitter.split_documents(docs)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5703b4-0995-41e7-96a8-f51cebb60b81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Use documents from a custom folder as Data Source\n",
    "In this example you will use the documents from the \"docs\" folder and create chunks from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e5b12-99ba-4a70-a6b3-f5ae9c1e6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    PyPDFium2Loader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a3469-7208-4326-b4ae-b7118a8c47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "def get_file_type(uploaded_item, supported_extensions=[\"pdf\", \"md\", \"txt\"]):\n",
    "    # Determine if uploaded_item is a file-like object or a string (path)\n",
    "    if hasattr(uploaded_item, \"name\"):\n",
    "        # It's a file-like object, extract the file name\n",
    "        file_name = uploaded_item.name\n",
    "    elif isinstance(uploaded_item, str):\n",
    "        # It's a string path, use it directly\n",
    "        file_name = uploaded_item\n",
    "    else:\n",
    "        raise ValueError(\"File extension not supported\")\n",
    "\n",
    "    print(\"filename: \", file_name)  # Debug print for file name\n",
    "\n",
    "    # Extract the file extension\n",
    "    extension = file_name.split(\".\")[-1].lower().strip()  # Added strip() to remove any trailing spaces\n",
    "    print(\"Extention: \", extension)  # Debug print for extension\n",
    "\n",
    "    # Check if the extension is in the supported list\n",
    "    if extension in supported_extensions:\n",
    "        return extension\n",
    "    else:\n",
    "        raise ValueError(\"File format not supported\")\n",
    "\n",
    "#function for chunking files\n",
    "def chunk_file(file_path, chunk_size=100, chunk_overlap = 20):\n",
    "    try:\n",
    "        # check if is a supported format\n",
    "        extension = get_file_type(file_path)\n",
    "\n",
    "        match extension:\n",
    "            case \"pdf\":\n",
    "                loader = PyPDFium2Loader(file_path)\n",
    "            case \"md\":\n",
    "                loader = UnstructuredMarkdownLoader(file_path)\n",
    "            case \"txt\":\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "            case _:\n",
    "                return \"File format not supported\"\n",
    "\n",
    "        pages = loader.load()\n",
    "        n_pages = len(pages)  # get the number of pages in the\n",
    "        print(\"Number of pages:\", n_pages)\n",
    "\n",
    "        # Split text in chunk using RecursiveCharacterTextSplitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.transform_documents(pages)\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d27fda-4984-43e7-9e24-e7ddbc0eb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting files from directory\n",
    "docs_path = \"docs/data\"\n",
    "folder_files = [\n",
    "        os.path.join(docs_path, f)\n",
    "        for f in os.listdir(docs_path)\n",
    "        if f.endswith(\".pdf\") or f.endswith(\".txt\") or f.endswith(\".md\")\n",
    "    ]\n",
    "#Create chunks and store them in \"data\"\n",
    "data=[]\n",
    "for idx, file_path in enumerate(folder_files):\n",
    "    print(f\"Processing file {idx + 1}/{len(folder_files)}: {file_path}\")\n",
    "    chunks = chunk_file(file_path, chunk_size=100, chunk_overlap = 20)\n",
    "    data.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90125986-623e-4149-bce4-a51e943f5e09",
   "metadata": {},
   "source": [
    "## Store chunks into a VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321d6be-e6fc-476d-b0f1-fb34baf0ddb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Store data in a ChromaDb\n",
    "We will store our chunks in a Vector DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d41803-abf4-47cc-a8bb-227cb9df50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac3f15-3cf0-4d5d-b9b8-c071d79750f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set apikey and embedding model from openai\n",
    "apikey = \"<OPENAI APIKEY>\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "#Set OpenAI Embedder\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embedding_model, openai_api_key=apikey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddc0b5-9388-4d3c-9f08-936479b220b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a persistent Chroma\n",
    "db_directory = \"db/chroma/data\"\n",
    "store = Chroma.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    ids=[f\"{item.metadata['source']}-{index}\" for index,item in enumerate(data)],\n",
    "    collection_name=\"CollectionName\",\n",
    "persist_directory=db_directory\n",
    ")\n",
    "store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efc19d-5197-47cc-9cb2-9afe5b202889",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Store data with Faiss\n",
    "We will store our chunks in a Vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852dc0a-4076-419e-a44f-da8c6068fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dc26f-9a63-4a6e-a6e8-fc111ef9cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set apikey and embedding model from openai\n",
    "apikey = \"<OPENAI APIKEY>\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "#Set OpenAI Embedder\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embedding_model, openai_api_key=apikey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10bb86-3009-4237-b399-d147b10bf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_directory = \"db/faiss/data\"\n",
    "store = FAISS.from_documents(data,embeddings)\n",
    "store.save_local(db_directory,index_name=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b230b1c-9765-4586-829a-a6feb90b3680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Asking Questions to the Virtual Assistant!\n",
    "Let's use OpenAI for answering our questions about information retrieved on Chroma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b02f4-f59a-4d9e-812d-7fa1d4023558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb8001-157e-413a-bdf9-1a9c6060b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize the general prompt template with wat you want\n",
    "template = \"\"\"\n",
    "                You are a Virtual Assistant that answers questions using only the context provided. \n",
    "                If there are multiple answers list them. Answer using the language of the question\n",
    "                {context}\n",
    "                Question: {question}\n",
    "                \"\"\"\n",
    "#Set up the prompt\n",
    "prompt = PromptTemplate(template = template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd8254-d5f1-408e-8056-1939de33a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the LLM\n",
    "llm = ChatOpenAI(temperature=0.8, model=\"gpt-4o\", openai_api_key=apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7efbfe-565d-43e3-94eb-a297ee083ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create our question/answer model passing the llm, chromaDB (or faiss) and the prompt.\n",
    "qa_with_source = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=store.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\":prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1c408-ae34-4804-9561-e7588df9b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(qa_with_source(\"Your question!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
